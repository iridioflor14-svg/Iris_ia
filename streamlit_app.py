# Arquivo: streamlit_app.py

import streamlit as st
import random
import numpy as np
import pickle
import os
import tensorflow as tf
import requests # Para fazer o download do Drive

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="IA IRIS: Intelig√™ncia Emocional e Criativa",
    page_icon="ü§ñ",
    layout="wide"
)

# SEU NOVO C√ìDIGO AQUI: SUBSTITUA PELA ID DO SEU ARQUIVO!
# ----------------------------------------------------------------
DRIVE_FILE_ID = '1TIdt1JJKTgahIsno9V3BkKcYQgKDzQ0e' # NOVO ID DO ARQUIVO .h5 NO GOOGLE DRIVE
# ----------------------------------------------------------------

# Nomes dos arquivos de Deep Learning
MODELO_ARQUIVO = 'modelo_sentimento.h5'
TOKENIZER_ARQUIVO = 'tokenizer.pkl'
MAX_LEN = 50 # O comprimento m√°ximo da sequ√™ncia usada no treinamento


# --- 1. Fun√ß√µes de Carregamento de Modelo (COM CACHE) ---

# Fun√ß√£o para baixar o arquivo do Google Drive
def download_file_from_google_drive(id, destination):
    URL = "https://docs.google.com/uc?export=download"
    session = requests.Session()

    response = session.get(URL, params = { 'id' : id }, stream = True)
    token = get_confirm_token(response)

    if token:
        params = { 'id' : id, 'confirm' : token }
        response = session.get(URL, params = params, stream = True)

    CHUNK_SIZE = 32768
    with open(destination, "wb") as f:
        for chunk in response.iter_content(CHUNK_SIZE):
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)
    return destination

def get_confirm_token(response):
    for key, value in response.cookies.items():
        if key.startswith('download_warning'):
            return value
    return None

@st.cache_resource
def carregar_modelo_e_tokenizer():
    """Carrega o modelo de sentimento e o tokenizer usando cache."""
    
    # Tenta baixar o modelo do Drive
    try:
        st.info(f"üåê Baixando o modelo {MODELO_ARQUIVO} do Google Drive...")
        download_file_from_google_drive(DRIVE_FILE_ID, MODELO_ARQUIVO)
            
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro ao baixar modelo do Drive. Verifique o ID e as permiss√µes de compartilhamento. Erro: {e}")
        return None, None, False

    # Tenta carregar o modelo e o tokenizer
    try:
        # O modelo .h5 AGORA EST√Å no diret√≥rio do Streamlit, baixado do Drive
        model = tf.keras.models.load_model(MODELO_ARQUIVO)

        # Carrega o tokenizer (Tenta carregar o .pkl, que √© pequeno e deve ter sido carregado)
        try:
             with open(TOKENIZER_ARQUIVO, 'rb') as f:
                tokenizer = pickle.load(f)
        except:
             # Se o tokenizer falhar (porque o upload falhou), usamos um tokenizer padr√£o para n√£o quebrar o app.
             from tensorflow.keras.preprocessing.text import Tokenizer
             # Este tokenizer √© um placeholder simples. A funcionalidade ser√° degradada, mas n√£o quebrada.
             tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)
             st.error("‚ùå O arquivo tokenizer.pkl n√£o foi encontrado. Usando um tokenizer padr√£o. A precis√£o da an√°lise pode ser afetada.")


        st.success("‚úÖ Modelo de Deep Learning carregado com sucesso!")
        return model, tokenizer, True
    
    except Exception as e:
        # Em caso de falha, volta para o modo simula√ß√£o.
        st.warning(f"‚ö†Ô∏è Erro ao carregar modelo local. Usando simula√ß√£o. Erro: {e}")
        return None, None, False


# --- 2. Fun√ß√µes de An√°lise de Sentimento (ID√äNTICAS) ---

def analisar_sentimento_real(frase, modelo, tokenizer):
    """Analisa o sentimento de uma frase usando o modelo de Deep Learning treinado."""
    sequence = tokenizer.texts_to_sequences([frase])
    # Se o tokenizer de emerg√™ncia for carregado, a sequ√™ncia pode ser vazia,
    # ent√£o usamos um pequeno check para evitar falhas.
    if not sequence or not sequence[0]:
        st.warning("Aviso: O tokenizer b√°sico n√£o conseguiu mapear a frase.")
    
    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(
        sequence, 
        maxlen=MAX_LEN, 
        padding='post', 
        truncating='post'
    )
    probabilidade = modelo.predict(padded_sequence)[0][0]
    return probabilidade


def simular_analise(frase):
    """Simula uma an√°lise de sentimento (usada quando o modelo n√£o √© carregado)."""
    P = random.uniform(0.1, 0.9)
    if P > 0.65:
        emocao = "Satisfa√ß√£o/Calma"
        resposta = "Sinto satisfa√ß√£o e calma. O resultado √© positivo."
        imagem = "Um lago tranquilo ao amanhecer, com n√©voa suave. Imagem."
    elif P < 0.35:
        emocao = "Desconforto/Tens√£o"
        resposta = "Detecto um forte desconforto. O resultado √© negativo."
        imagem = "Uma tempestade que se aproxima em um deserto. Imagem."
    else:
        emocao = "Ambiguidade/Neutro"
        resposta = "Estou confusa. O sentimento √© amb√≠guo."
        imagem = "Uma interroga√ß√£o gigante flutuando em um nevoeiro cinzento. Imagem."
    return P, emocao, resposta, imagem


def fazer_analise_e_resposta(frase, modelo, tokenizer, modelo_carregado):
    """Decide se usa o modelo real ou a simula√ß√£o e formata a sa√≠da."""
    
    if modelo_carregado:
        # Usa o modelo de Deep Learning real!
        P_real = analisar_sentimento_real(frase, modelo, tokenizer)
        
        # Traduz a pontua√ß√£o do modelo real (P_real) em uma resposta para a IRIS
        if P_real > 0.7:
            emocao = "Felicidade/Satisfa√ß√£o"
            resposta = "Detecto uma forte **emo√ß√£o positiva**! Meu sentimento √© de satisfa√ß√£o."
            imagem = "Um campo de flores sob o sol. Imagem."
        elif P_real < 0.3:
            emocao = "Tristeza/Tens√£o"
            resposta = "Sinto uma **emo√ß√£o negativa** clara. Detecto um sentimento de tens√£o ou tristeza."
            imagem = "O reflexo de uma l√°grima em uma po√ßa. Imagem."
        else:
            emocao = "Neutro/Ambiguidade"
            resposta = "O sentimento √© **neutro ou amb√≠guo**. N√£o consigo classificar a emo√ß√£o de forma clara."
            imagem = "Uma parede branca e lisa. Imagem."
            
        P = P_real
        return P, emocao, resposta, imagem
    
    else:
        # Usa a simula√ß√£o (se o modelo real falhar)
        return simular_analise(frase)


# --- 3. Fun√ß√£o de Busca e Tradu√ß√£o (Simuladas) ---

def simular_busca_e_traducao(frase, resultado_sentimento):
    """Simula a busca na web e a tradu√ß√£o."""
    
    # 3.1 Simula√ß√£o de Busca
    if len(frase.split()) > 5:
        st.info("üåê A IA IRIS iniciaria uma busca na web com a frase...")
    
    # 3.2 Simula√ß√£o de Tradu√ß√£o
    traducao = f"The translation service is active but running in a simulated mode for this response type."
    
    return traducao


# --- 4. Interface Principal do Streamlit ---

def main():
    
    st.title("IA IRIS: Intelig√™ncia Emocional e Criativa ü§ñ")
    st.markdown("---")
    
    # Carrega o modelo de sentimento e o tokenizer uma √∫nica vez
    modelo, tokenizer, modelo_carregado = carregar_modelo_e_tokenizer()

    st.markdown("Diga √† Iris para 'escrever um poema', 'quem √© o criador', ou apenas uma frase (ex: 'Sinto-me muito feliz hoje').")

    # √Årea de entrada de texto
    frase_usuario = st.text_input("Sua Frase para a IA Iris:", value="Qual seu nome?")

    # Bot√£o de A√ß√£o (√∫nico bot√£o para todas as a√ß√µes)
    if st.button("Analisar Sentimento, Fazer Busca e Traduzir", type="primary"):
        
        if not frase_usuario.strip():
            st.warning("Por favor, digite uma frase para a IA Iris analisar.")
            return

        # --- Execu√ß√£o da An√°lise de Sentimento (Real ou Simulado) ---
        
        # Retorna P (pontua√ß√£o), emo√ß√£o, resposta_iris e imagem_desc
        P, emocao, resposta_iris, imagem_desc = fazer_analise_e_resposta(
            frase_usuario, modelo, tokenizer, modelo_carregado
        )

        st.markdown("---")
        st.subheader("üëÅÔ∏è Resultado da An√°lise da IA Iris:")

        # Exibi√ß√£o do Resultado
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.metric(label="Pontua√ß√£o P (Probabilidade)", value=f"{P:.2f}")

        with col2:
            st.write(f"**{emocao}:** {resposta_iris} (P: {P:.2f})")
            st.write(f"**Express√£o Visual:** {imagem_desc}")
            
        # --- Execu√ß√£o da Busca e Tradu√ß√£o (Simuladas) ---
        traducao = simular_busca_e_traducao(frase_usuario, resposta_iris)

        st.markdown("---")
        st.subheader("üìö Tradu√ß√£o (Ingl√™s):")
        st.info(traducao)

        # Mensagem de Sucesso no Final
        if modelo_carregado:
            st.success("‚úÖ A IA IRIS demonstrou todo seu potencial de Deep Learning real, busca na web e tradu√ß√£o!")
        else:
            st.success("‚úÖ A IA IRIS demonstrou todo seu potencial de ML (simulado), busca na web e tradu√ß√£o!")

if __name__ == "__main__":
    main()